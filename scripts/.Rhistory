group2 = c("Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8")
data.table = cbind(group1,group2)
data.table = cbind(group1, group2)
dimnames(data.table)=list("Patinet ID"=c("P1", "P2", "P3", "P4", "P5", "P6", "P7", "P8", "P9", "P10"),"Vars"=c("Wellness","Sleep amount"))
sleep <- data.frame(data.table)
sleep$Wellness = as.numeric(levels(sleep$Wellness))[sleep$Wellness]
sleep
group1 = c(2.3, 3.3, 3.2, 3.2, 3.2, 2.1, 2.0, 1.8, 0.4, 0.3)
group2 = c("Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8")
data.table = cbind(group1,group2)
data.table = cbind(group1, group2)
dimnames(data.table)=list("Patinet ID"=c("P1", "P2", "P3", "P4", "P5", "P6", "P7", "P8", "P9", "P10"),"Vars"=c("Wellness","Sleep amount"))
sleep <- data.frame(data.table)
sleep$Wellness = as.numeric(levels(sleep$Wellness))[sleep$Wellness]
sleep
group1 = c(2.3, 3.3, 3.2, 3.2, 3.2, 2.1, 2.0, 1.8, 0.4, 0.3)
group2 = c("Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8")
data.table = cbind(group1,group2)
data.table = cbind(group1, group2)
dimnames(data.table)=list("Patinet ID"=c("P1", "P2", "P3", "P4", "P5", "P6", "P7", "P8", "P9", "P10"),"Vars"=c("Wellness","Sleep amount"))
sleep <- data.frame(data.table)
sleep$Wellness = as.numeric(levels(sleep$Wellness))[sleep$Wellness]
sleep
group1 = c(2.3, 3.3, 3.2, 3.2, 3.2, 2.1, 2.0, 1.8, 0.4, 0.3)
group2 = c("Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Greater than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8", "Smaller than 8")
data.table = cbind(group1,group2)
data.table = cbind(group1, group2)
dimnames(data.table)=list("Patinet ID"=c("P1", "P2", "P3", "P4", "P5", "P6", "P7", "P8", "P9", "P10"),"Vars"=c("Wellness","Sleep amount"))
sleep <- data.frame(data.table)
sleep$Wellness = as.numeric(levels(sleep$Wellness))[sleep$Wellness]
sleep
---
findAssocs(dtm, "love", 0.95)[1:10]
findAssocs(dtm, "love", 0.95)[1:10]
library(tm)
shakespeare <- VCorpus(DirSource("data", encoding = "UTF-8"))
shakespeare
# Remove whitespace
shakespeare <- tm_map(shakespeare, stripWhitespace, lazy=TRUE)
# Stemming
shakespeare <- tm_map(shakespeare, stemDocument, lazy=TRUE)
# Remove punctuation
shakespeare <- tm_map(shakespeare, removePunctuation, lazy=TRUE)
findAssocs(dtm, "love", 0.95)[1:10]
dtm <- DocumentTermMatrix(shakespeare)
findAssocs(dtm, "love", 0.95)[1:10]
loves_assocs <- findAssocs(dtm, "love", 0.95)
freq <- sort(colSums(as.matrix(dtms)),decreasing=TRUE)
library(wordcloud)
set.seed(123)
freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)
library(wordcloud)
set.seed(123)
freq
freq[1:20]
library(wordcloud)
set.seed(123)
wordcloud(names(freq),freq,min.freq=4000,colors=brewer.pal(6,"Dark2"))
freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)
library(wordcloud)
set.seed(123)
wordcloud(names(freq),freq,min.freq=3800,colors=brewer.pal(6,"Dark2"))
freq[1:30]
wordcloud(names(freq),freq,min.freq=3000,colors=brewer.pal(6,"Dark2"))
wordcloud(names(freq),freq,min.freq=3000)
wordcloud(names(freq),freq,min.freq=3000, colors=brewer.pal(8, "Dark2"))
wordcloud(names(freq),freq,min.freq=2800, colors=brewer.pal(8, "Dark2"))
wordcloud(names(freq),freq, min.freq=2800, max.words = 100, colors=brewer.pal(8, "Dark2"))
wordcloud(names(freq),freq, min.freq=2800, max.words = 5, colors=brewer.pal(8, "Dark2"))
wordcloud(names(freq),freq, min.freq=2500, max.words = 100, colors=brewer.pal(8, "Dark2"))
library(wordcloud)
set.seed(123)
wordcloud(names(freq), freq, min.freq=2500, max.words = 100, colors=brewer.pal(8, "Dark2"))
termMatrix <- as.matrix(dtm)
termMatrix[termMatrix>=1] <- 1
termMatrix[5:10,5:10]
termMatrix
install.packages("igraph"")
''
""
install.packages("igraph)
install.packages("igraph):
install.packages("igraph")
library(igraph)
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
g <- simplify(g)
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g)
plot(g, layout=layout1)
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
dtm[5:10, 1:3]
dtm[1:3, 1:3]
dtm[1:3, 1:5]
dtm[1:3, 1:100]
loves_assocs
class(loves_assocs)
hist(loves_assocs)
plot(loves_assocs)
freq
class(freq)
freq <- sort(colSums(as.matrix(dtm)),decreasing=TRUE)
class(freq)
freq
barplot(freq[1:50])
?barplot
barplot(freq[1:50], xlab = "term", ylab = "frequency",  col=topo.colors(12))
barplot(freq[1:50], xlab = "term", ylab = "frequency",  col=heat.colors(12))
barplot(freq[1:50], xlab = "term", ylab = "frequency",  col=heat.colors(50))
barplot(freq[1:50], xlab = "term", ylab = "frequency",  col=heat.colors(50, 0.8))
barplot(freq[1:30], xlab = "term", ylab = "frequency",  col=heat.colors(30))
barplot(freq[1:40], xlab = "term", ylab = "frequency",  col=heat.colors(40))
barplot(freq[1:1000], xlab = "term", ylab = "frequency",  col=heat.colors(1000))
barplot(freq[1:200], xlab = "term", ylab = "frequency",  col=heat.colors(200))
barplot(freq[1:100], xlab = "term", ylab = "frequency",  col=heat.colors(100))
barplot(freq[1:30], xlab = "term", ylab = "frequency",  col=heat.colors(30))
barplot(freq[1:20], xlab = "term", ylab = "frequency",  col=heat.colors(20))
source('~/angerhang.github.io/statsWithR/src/chaper2/style.R')
source('~/angerhang.github.io/statsWithR/src/style.R')
source('~/angerhang.github.io/statsWithR/src/style.R')
source('~/angerhang.github.io/statsWithR/src/style.R')
source('~/angerhang.github.io/statsWithR/src/chaper2/style.R')
source('~/angerhang.github.io/statsWithR/src/chaper2/style.R')
library(devtools)
install_github('rstudio/rmarkdown')
install.packages('knitr', repos = c('http://rforge.net', 'http://cran.rstudio.org'),
type = 'source')
install_github('jimhester/knitrBootstrap')
install.packages("twitteR")
library(twitteR)
setup_twitter_oauth("4tpxISsUHdWakcxH0M0VM3Dkl", "xfxHdjywUdy8CPFByHFuI0YKqhLxF2rK9fSHtmWnliMZGepqsP")
setup_twitter_oauth("4tpxISsUHdWakcxH0M0VM3Dkl", "xfxHdjywUdy8CPFByHFuI0YKqhLxF2rK9fSHtmWnliMZGepqsP")
setup_twitter_oauth("4tpxISsUHdWakcxH0M0VM3Dkl", "xfxHdjywUdy8CPFByHFuI0YKqhLxF2rK9fSHtmWnliMZGepqsP")
setup_twitter_oauth("4tpxISsUHdWakcxH0M0VM3Dkl", "xfxHdjywUdy8CPFByHFuI0YKqhLxF2rK9fSHtmWnliMZGepqsP")
tweets <- searchTwitter('#rstats', n=50)
tweet()
tweets
head(tweets)
tweets <- searchTwitter('#AI', n=40)
head(tweets)
jacobs <- getUser('Jacobs University')
jacobs <- getUser('Jacobs University')
jacobs <- getUser('Jacobs\ University')
jacobs <- getUser('Jacobs\ University')
jacobs <- getUser('jacobs_bremen')
jacobs$getDescription()
jacobs$getFavorites(n=10)
myDf <- twListToDF(tweets)
head(myDf)
jacobs_tweets <- userTimeline('jacobs_bremen')
jacobs_tweets[1:3]
availableTrendLocations()
trends = getTrends(2367105)
trends = getTrends(1)
head(trends)
friendships
friendships("angerhang", "jacobs_bremen")
dT <- getUser('realDonaldTrump')
follower <- dT$getFollowers()
dT <- userTimeline('realDonaldTrump')
?userTimeline
dT <- userTimeline('realDonaldTrump', n = 100)
n
head(dT)
dT <- userTimeline('realDonaldTrump', n = 3200)
library(tm)
dT_tm <- VCorpus(VectorSource(dT))
head(dT)
kk = unlist(dT)
head(kk)
dT_tm <- VCorpus(VectorSource(kK))
dT_tm <- VCorpus(VectorSource(kk))
rapply(dT, c)
kk = rapply(dT, c)
class(kk)
kk = do.call(c, dT)
class(kk)
kk = do.call("cbind", dT)
dT(1)
dT[1]
dT[1][1]
kk = unlist(dT)
kk = unlist(dT[1])
kk
str(kk)
kk$getFavoriteCount
kk$favoriteCount
kk[1]$favoriteCount
myDf <- ttwListToDF(dT)
myDf <- twListToDF(dT)
View(myDf)
plot(myDf$created, myDf$favoriteCount)
class(myDf$created)
myDf$date <- as.Date(myDf$created, '%Y-%d-%m')
as.Date()
?(as.Date)
?as.Date
myDf$created[1]
as.Date(myDf$created[1])
plot(myDf$created, myDf$favoriteCount)
plot.ts(myDf$created, myDf$favoriteCount)
plot(myDf$created, myDf$favoriteCount)
install.packages("tm.plugin.webmining")
library(tm)
library(tm.plugin.webmining)
jacobs <- Corpus(GoogleNewsSource("Germany"))
?GoogleNewsSource
corpus <- Corpus(GoogleNewsSource("Microsoft"))
corpus <- WebCorpus(GoogleNewsSource("Microsoft"))
class(corpus)
corpus
meta(corpus[[1]])
corpus[[1]]
corpus[[1]]$content
corpus[[2]]$content
corpus[[3]]$content
corpus[1]$content
corpus[1]$content
corpus[1]
inspect(Corpus())
inspect(corpus)
?extractContentDOM
extractContentDOM("http://www.economist.com/news/science-and-technology/21698641-new-way-understand-behaviour-ice-sheets-good-vibrations")
extractContentDOM("http://www.economist.com/news/science-and-technology/21698641-new-way-understand-behaviour-ice-sheets-good-vibrations")
my = url("http://www.economist.com/news/science-and-technology/21698641-new-way-understand-behaviour-ice-sheets-good-vibrations")
extractContentDOM(my)
test1 <- extractContentDOM("http://tecfa.unige.ch",0.1,FALSE)
test1
summary(corpus)
?extractContentDOM
climateArc <- extractContentDOM("http://www.economist.com/news/science-and-technology/21698641-new-way-understand-behaviour-ice-sheets-good-vibrations",0.5,FALSE)
climateArc
climateArc <- trimWhiteSpaces(climateArc)
climateArc
install.packages("RTextTools")
library('RTextTools')
data("USCongress")
my_matrix <- create_matrix(USCongress$text, language = "english", stemWords = TRUE,
removeSparseTerms = .999, removeNumbers = TRUE)
USCongress
View(USCongress)
summary(USCongress)
str(USCongress)
?USCongress
my_svm <- train_model(my_container, algorithm = "SVM")
my_nn <- train_model(my_container, algorithm = "NNET")
my_container <- create_container(my_matrix, USCongress$major, trainSize = 1:4000,
testSize = 4001:4449, virgin = FALSE)
my_svm <- train_model(my_container, algorithm = "SVM")
my_nn <- train_model(my_container, algorithm = "NNET")
svm_predictions <- classify_model(my_container, algorithm = "SVM")
svm_predictions <- classify_model(my_container, SVM)
svm_predictions <- classify_model(my_container, model = "SVM")
?classify_model
my_svm <- train_model(my_container, algorithm = "SVM")
svm_predictions <- classify_model(my_container, model = "SVM")
svm_predictions <- classify_model(my_container, model = my_svm)
my_rf <- train_model(my_container, algorithm = "RF")
?train_model
my_boost <- train_model(my_container, algorithm = "BOOSTING")
analytics <- create_analytics(my_container, cbind(svm_predictions))
summary(analytics)
install.packages("RMySQL")
---
mysqlconnection = dbConnect(MySQL(), user = 'root', password = '', dbname = 'sakila',
host = 'localhost')
# List the tables available in this database.
dbListTables(mysqlconnection)
library(DBI)
mysqlconnection = dbConnect(MySQL(), user = 'root', password = '', dbname = 'sakila',
host = 'localhost')
?dbConnect
library(RMySQL)
dbConnect(RMySQL::MySQL(),  user = 'root', password = '', dbname = 'sakila',
+                             host = 'localhost')
con <- dbConnect(RMySQL::MySQL(), dbname = "test")
con <- dbConnect(RMySQL::MySQL(), dbname = "test")
?"dbConnect"
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), dbname = "test")
}
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), dbname = "test", host = 'localhost')
}
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), dbname = "test")
}
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), dbname = "test")
}
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), group = "rs-dbi")
}
con <- dbConnect(RMySQL::MySQL(), dbname="mysql")
dbListTables(con)
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), group = "rs-dbi")
}
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), group = "rs-dbi")
}
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), dbname = "mysql")
}
con <- dbConnect(RMySQL::MySQL(), dbname="mysql")
con <- dbConnect(RMySQL::MySQL(), dbname="test")
if (mysqlHasDefault()) {
con <- dbConnect(RMySQL::MySQL(), group="test")
}
con <- dbConnect(RMySQL::MySQL(), group="test")
dbListConnections()
dbListTables(conn = )
dbListTables(con )
dbListTables(con)
con <- dbConnect(RMySQL::MySQL(), group="test")
dbListTables(con)
dbDisconnect(con)
summary(con, verbose = TRUE)
con <- dbConnect(RMySQL::MySQL(), dbName="mysql")
con <- dbConnect(RMySQL::MySQL(), dbName="mysql")
dbListTables(con)
dbListFields(con, "table\user")
dbListFields(con, "user")
dbListTables(con)
d <- dbReadTable(con, "WL")
d <- dbReadTable(con, "time_zone")
d <- dbReadTable(con, "user")
View(d)
users <- dbReadTable(con, "user")
summary(users)
str(users)
user(1)
user[1]
users[1]
users[2]
class(users)
str(users)
options(warn=-1)
options(warn=0)
library(knitr)
?"knit"
dbGetQuery(con, "select * from mysql")
dbListTables(con)
dbGetQuery(con, "select * from user")
dbGetQuery(con, "select * from time_zone")
dbGetQuery(con, "select * from Host")
dbGetQuery(con, "select * from event")
dbGetQuery(con, "select * from servers")
dbGetQuery(con, "select * from time_zone_name")
summary(con)
summary(con, verbose = TRUE)
dbListConnections(MySQL())
summary(MySQL())
summary(MySQL(), verbose = TRUE)
dbGetRowCount(con)
show(con)
?show
myUsers <- dbGetQuery(con, "select * from user")
dbGetRowCount(myUsers)
rs <- dbSendQuery(con, "SELECT * FROM mysql WHERE user == root")
rs <- dbSendQuery(con, "SELECT * FROM mysql WHERE user = root")
rs <- dbSendQuery(con, "SELECT * FROM user WHERE user = root")
users <- dbReadTable(con, "user")
users
rs <- dbSendQuery(con, "SELECT * FROM user WHERE User like 'root'")
dbHasCompleted(rs)
dbGetStatement(rs)
dbHasCompleted(rs)
dbGetInfo(rs)
dbClearResult(rs)
myData <- read.csv("Desktop/good journey.csv")
View(myData)
# Add zeros
myDate <- myData$Date
newData <- lapply(myDate, function(x) class(x))
lapply(myDate, function(x) class(x))
myDate <- as.character(myData$Date)
newData <- lapply(myDate, function(x) class(x))
lapply(myDate, function(x) class(x))
install.packages(c("BH", "cluster", "corrplot", "devtools", "evaluate", "formatR", "git2r", "highr", "httr", "jsonlite", "knitr", "lazyeval", "Matrix", "multcomp", "nlme", "openssl", "plyr", "Rcpp", "rmarkdown", "rsconnect", "shinyjs", "slam", "stringi", "survival", "tidyr", "withr", "zoo"))
a = "123"
substr(a, 1, 2)
newData <- lapply(myDate, function(x) if (nchar(x) == 0) paste("0", x) else x)
head(newData)
myDate <- as.character(newData)
myDate
newData
newData <- lapply(myDate, function(x) if (nchar(x) == 8) paste("0", x) else x)
newData
newData <- lapply(myDate, function(x) if (nchar(x) == 8) paste("0", x, sep ="") else x)
myDate <- as.character(newData)
# Add zeros
myDate <- as.character(myData$Date)
newData <- lapply(myDate, function(x) if (nchar(x) == 8) paste("0", x, sep ="") else x)
myDate <- as.character(newData)
newDate <- lapply(myDate, function(x) paste(substr(x, 8, 9), substr(x, 1, nchar(x) -2), sep="")
newDate <- lapply(myDate, function(x) paste(substr(x, 8, 9), substr(x, 1, nchar(x) -2), sep=""))
newDate <- lapply(myDate, function(x) paste(substr(x, 8, 9), substr(x, 1, nchar(x) -2), sep=""))
# Switch
newDate <- lapply(myDate, function(x) paste(substr(x, 8, 9),
substr(x, 3, 7),
substr(x, 1, 2), sep=""))
finalDate <- lapply(newDate, function(x) paste("20", x, sep=""))
myData$Date <- finalDate
write.csv(myData, "try again.csv")
class(myData$Date)
myData$Date <- as.character(myData$Date)
write.csv(myData, "try again.csv")
myData <- read.csv("Desktop/final travel time.csv")
View(myData)
myData$X <- NULL
myData$X.1 <- NULL
View(myData)
summary(myData$Type.Transportation)
myData[myData$Type.Transportation =="National Rail"]
myData[myData$Type.Transportation =="National Rail", ]
mine <- myData[myData$Type.Transportation =="National Rail", ]
summary(mine$Type.Transportation)
hist(mine$Duration)
View(mine)
library(rgdal)
install.packages("rgdal")
library("rgdal")
stations <- readOGR(dsn="data", layer="lnd-stns")
install.packages("maptools")
install.packages("tmap")
install.packages("rgeos")
install.packages("ggmap")
#1
getNetwrok(badData)
library(dplyr)
library(jsonlite)
source('~/Desktop/HPIHack-master/getData.R')
dpois(1, 3)
dpois(1, 1)
dpois(0, 1)
dpois(0, 1000)
dpois(999, 1000)
x = 0
for (i in 1:999){
x += dpois(i, 1000)
}
x = 0
for (i in 1:999){
x = x +  dpois(i, 1000)
}
x
e
exp
exp(1)
exp(-1000)
help dpois()
dpois()
dpois()?
?
//
?dpois
dpois(0, 1000)
dpois(1, 1000)
x = 0
for (i in 0:999){
x = x +  dpois(i, 1000)
}
x
dpois(1000, 1000)
999000 - 1000 * 1000
-1000/sqrt(1000 * 1000)
x = 0
for (i in 0:999000){
x = x +  dpois(i, 1000000)
}
x
x = 0
for (i in 0:999000){
x = x +  dpois(i, 1000000)
}
x
clear
library(RCurl)
url <- ('http://www.stat.cmu.edu/tartandatasciencecup/episodeII/transaction_data_8451.csv')
data <- getURI(url)
data
data <- read.csv(textConnection(datafile), header=T)
library(RCurl)
url <- ('http://www.stat.cmu.edu/tartandatasciencecup/episodeII/transaction_data_8451.csv')
datafile <- getURI(url)
data <- read.csv(textConnection(datafile), header=T)
View(data)
head(data)
summary(data)
library(RCurl)
url <- ('http://www.stat.cmu.edu/tartandatasciencecup/episodeII/transaction_data_8451.csv')
datafile <- getURI(url)
data <- read.csv(textConnection(datafile), header=T)
write.csv(data, file = '../data/rawata.csv')
myData <- read.csv('../data/rawData.csv')
## normalizae day
myData$DAY <- myData$DAY - min(myData$DAY)
## remove quantity with value 0
myData <- myData[myData$QUANTITY != 0 ,]
## Add price per product
myData$PRICE_PER_PRODUCT = myData$BASE_SPEND_AMT / myData$QUANTITY
## Household demographics data
householdData <- myData[! duplicated(myData$household_key) ,]
## ToDo(Phill) time series for household shopping
write.csv(myData, '../data/cleanData.csv')
write.csv(householdData, '../data/householdData.csv')
source('~/cmuDSC/scripts/cleanData.R')
source('~/cmuDSC/scripts/getData.R')
source('~/cmuDSC/scripts/cleanData.R')
source('~/cmuDSC/scripts/cleanData.R')
